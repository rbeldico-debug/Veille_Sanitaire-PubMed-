### Structure globale des ADR : 

1.  **ADR-001 : Architecture en Couches (Layered Architecture)**
2.  **ADR-002 : Gestion et Validation des Données avec Pydantic**
3.  **ADR-003 : Stratégie d'Interaction avec PubMed (BioPython)**
4.  **ADR-004 : Abstraction du Service de Résumé IA (LLM)**
5.  **ADR-005 : Persistance des Données (Google Sheets comme Repository)**
6.  **ADR-006 : Stratégie d'Exécution Automatique au Démarrage**

---

### Détail des ADRs

#### ADR-001 : Architecture en Couches (Layered Architecture)

*   **Statut** : Proposé
*   **Contexte** : Le workflow n8n mélangeait la logique métier (filtrer les articles), l'accès aux données (appels HTTP, Google Sheets) et la présentation. Pour éviter le problème de "difficulté d'emboîtement" et de maintenance mentionné, nous devons séparer les responsabilités.
*   **Décision** : Adopter une architecture modulaire en couches :
    1.  **Domain/Model** : Les objets de données (Pydantic).
    2.  **Services** : La logique métier (Orchestrateur de veille, Comparateur de dates/ID, Nettoyeur de texte).
    3.  **Infrastructure/Adapters** : Les connexions externes (Client PubMed, Client Google Sheets, Client Ollama).
    4.  **Main/Entrypoint** : Le script de lancement.
*   **Conséquences** :
    *   (+) **Modularité** : On peut changer de fournisseur LLM ou de base de données sans casser la logique de récupération PubMed.
    *   (+) **Testabilité** : On peut tester la logique de filtrage sans faire de vrais appels API.
    *   (-) Demande un peu plus de code initial ("boilerplate") qu'un simple script linéaire.

#### ADR-002 : Gestion et Validation des Données avec Pydantic

*   **Statut** : Accepté (Demandé par le client)
*   **Contexte** : La "mauvaise gestion des données" dans n8n provenait probablement de types dynamiques (JSON non strict). Nous avons besoin de garantir que les données (Article, Résumé, Date) sont valides à tout moment.
*   **Décision** : Utiliser **Pydantic V2** pour modéliser toutes les entités transitant dans le système.
    *   Exemple de modèles : `PubMedArticle`, `EnrichedArticle`, `GoogleSheetRow`.
*   **Conséquences** :
    *   (+) **Sécurité** : Le programme plantera immédiatement si l'API PubMed change de format, au lieu de propager des données corrompues.
    *   (+) **Autocomplétion** : Dans PyCharm, vous aurez l'autocomplétion sur `article.title`, `article.pub_date`, etc.
    *   (+) **Facilité de parsing** : Conversion automatique JSON -> Objet Python.

#### ADR-003 : Stratégie d'Interaction avec PubMed

*   **Statut** : Proposé
*   **Contexte** : Le workflow actuel fait des requêtes HTTP brutes (`esearch`, `efetch`) et parse du XML manuellement. C'est fragile et verbeux.
*   **Décision** : Utiliser la bibliothèque **Biopython** (`Bio.Entrez`).
*   **Décision Alternative écartée** : `requests` + `xml.etree` (Trop bas niveau, réinvention de la roue pour gérer les rate-limits de NCBI).
*   **Conséquences** :
    *   (+) **Fiabilité** : Biopython gère nativement les spécificités de l'API Entrez (NCBI) et le parsing XML complexe.
    *   (+) **Conformité** : Facilite le respect des conditions d'utilisation de NCBI (email, tool name).

#### ADR-004 : Abstraction du Service de Résumé IA

*   **Statut** : Proposé
*   **Contexte** : Vous utilisez actuellement Ollama via n8n. Le paysage de l'IA change vite (nouveaux modèles, passage potentiel à une API distante comme OpenAI ou Mistral si la machine locale est surchargée).
*   **Décision** : Créer une interface (classe abstraite) `SummarizationService` avec une implémentation `OllamaLocalService`. Utiliser la librairie `langchain` ou `ollama-python` pour l'implémentation.
*   **Conséquences** :
    *   (+) **Flexibilité** : Si demain vous voulez utiliser GPT-4 ou Claude à la place d'Ollama, il suffira de changer une ligne de configuration, sans toucher au reste du code.
    *   (+) **Isolation du Prompt** : Le prompt système ("Tu es un expert...") sera géré dans le code, versionné avec Git.

#### ADR-005 : Persistance des Données (Google Sheets comme Repository)

*   **Statut** : Proposé
*   **Contexte** : La sortie actuelle est un Google Sheet. C'est pratique pour la consultation humaine, mais moins pour la gestion d'état (savoir quels articles ont déjà été traités).
*   **Décision** : Maintenir Google Sheets comme sortie principale pour l'utilisateur, mais abstraire son accès via le pattern **Repository**. Utiliser la librairie `gspread` pour l'interaction.
    *   *Note* : À terme, il faudra envisager une petite base de données locale (SQLite) pour stocker les IDs déjà traités ("Cache"), afin d'éviter de lire tout le Google Sheet à chaque exécution pour déduire les doublons.
*   **Conséquences** :
    *   (+) Maintient l'usage actuel des utilisateurs finaux.
    *   (-) Google Sheets est lent comme base de données.

#### ADR-006 : Stratégie d'Exécution Automatique au Démarrage

*   **Statut** : Proposé
*   **Contexte** : Le programme doit se lancer au démarrage du PC.
    *   *Point technique* : PyCharm est un outil de développement (IDE). Il ne sert pas à "lancer" des programmes en production ou au démarrage de la machine.
*   **Décision** :
    1.  Le programme sera packagé sous forme de script Python exécutable (ou via un fichier `.bat` / `.sh` qui lance l'environnement virtuel).
    2.  L'automatisation sera gérée par le **Planificateur de tâches (Windows)** ou **Cron/Systemd (Linux/Mac)**, et non par PyCharm.
*   **Conséquences** :
    *   (+) **Indépendance** : Le script tourne même si PyCharm n'est pas ouvert.
    *   (+) **Robustesse** : Le système d'exploitation gère le lancement.

---

### Analyse de l'existant sur GitHub (Phase 1)

Avant de créer le repository, comme vous l'avez suggéré, je vous recommande de chercher avec les mots-clés suivants. Cependant, votre besoin étant très spécifique (PubMed -> Ollama -> GSheets avec logique de dédoublonnement), il est très probable que nous devions créer notre propre solution ("Glue code").

**Mots clés de recherche :**
*   `python pubmed monitoring`
*   `pubmed to google sheets python`
*   `biopython llm summary`

### Structure de projet recommandée (Phase 2)

Si nous validons la création du repo, voici la structure alignée avec les ADRs :

```text
sanitary-watch/
├── data/                   # Cache local (SQLite ou JSON) pour les IDs traités
├── src/
│   ├── domain/
│   │   ├── models.py       # Classes Pydantic (Article, Summary)
│   │   └── exceptions.py
│   ├── services/
│   │   ├── watcher.py      # Orchestrateur (Logique principale)
│   │   └── cleaner.py      # Nettoyage HTML/XML
│   ├── infra/
│   │   ├── pubmed_client.py # Wrapper Biopython
│   │   ├── gsheets_repo.py  # Wrapper gspread
│   │   └── ai_client.py     # Wrapper Ollama
│   ├── main.py             # Point d'entrée
│   └── config.py           # Gestion des variables d'env (API Keys, Sheet ID)
├── tests/
├── .env                    # Secrets (ne pas commiter sur GitHub)
├── requirements.txt
└── README.md
```